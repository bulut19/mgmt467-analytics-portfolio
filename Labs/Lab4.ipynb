{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bulut19/mgmt467-analytics-portfolio/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCFP5SIsYFjH"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "dCFP5SIsYFjH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8z0DXlOYFjR"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "W8z0DXlOYFjR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXJF33maYFjT"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "wXJF33maYFjT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWaZ-91kYFjV"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "JWaZ-91kYFjV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh1AJxnyYFjW"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "nh1AJxnyYFjW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDjYO88LYFjX"
      },
      "execution_count": 3,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "# # from google.colab import auth\n",
        "# # auth.authenticate_user()\n",
        "# #\n",
        "# # import os\n",
        "# # PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "# # REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "# # os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "# # print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "# #\n",
        "# # # Set active project for gcloud/BigQuery CLI\n",
        "# # !gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# # !gcloud config get-value project\n",
        "# # # Done: Auth + Project/Region set"
      ],
      "id": "BDjYO88LYFjX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9f19a49",
        "outputId": "addc55fc-8b31-49fb-cf62-7bc1ebe8cac6"
      },
      "source": [
        "# Authenticates the Colab environment to Google Cloud, allowing access to GCP services.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Prompts the user to enter their Google Cloud Project ID.\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "# Sets the default region for Google Cloud services.\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "# Exports the PROJECT_ID and REGION as environment variables.\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"REGION\"] = REGION\n",
        "\n",
        "\n",
        "# Sets the active project for the gcloud command-line tool.\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# Prints the configured project and region for verification.\n",
        "print(f\"Project: {PROJECT_ID} | Region: {REGION}\")\n",
        "\n",
        "# Done: Auth + Project/Region set"
      ],
      "id": "c9f19a49",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: boxwood-veld-471119-r6\n",
            "Updated property [core/project].\n",
            "Project: boxwood-veld-471119-r6 | Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSV7FASlYFjZ"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "mSV7FASlYFjZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ffd24f",
        "outputId": "19646d2f-46cd-4b4a-e300-c4ddf353fc9e"
      },
      "source": [
        "# Print the active project\n",
        "!gcloud config get-value project\n",
        "\n",
        "# Echo the set region\n",
        "import os\n",
        "print(f\"Region: {os.environ.get('REGION')}\")"
      ],
      "id": "e3ffd24f",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "boxwood-veld-471119-r6\n",
            "Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkWqhRwYFja"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?\n",
        "\n",
        "For consistency, to make sure all later commands use the same project and region. If not, commands might default to wrong projects or regions, causing errors when resources aren't found where expected, creating resources in wrong locations, and issues if reproducibility when you try to run the code in a different environment."
      ],
      "id": "zBkWqhRwYFja"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOf6v5YYFjb"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "atOf6v5YYFjb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttw35IQtYFjc"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "Ttw35IQtYFjc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAdbxkzjYFjc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "# # from google.colab import files\n",
        "# # print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "# # uploaded = files.upload()\n",
        "# #\n",
        "# # import os\n",
        "# # os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# # with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "# #     f.write(uploaded[list(uploaded.keys())[0]])\n",
        "# # os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "# #\n",
        "# # !kaggle --version"
      ],
      "id": "mAdbxkzjYFjc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt the user to upload their kaggle.json file\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create the .kaggle directory and save the kaggle.json file\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "\n",
        "# Set restrictive permissions (owner-only read/write) for security\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "\n",
        "# Verify the Kaggle installation by printing the version\n",
        "!kaggle --version\n",
        "\n",
        "# This setup ensures your Kaggle API key is stored securely and the environment is ready for reproducible downloads."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "RaEtGlt8bQ3F",
        "outputId": "8d29e9f7-7ea2-468b-dd1d-f76be39960a4"
      },
      "id": "RaEtGlt8bQ3F",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-196c9471-8526-4ca5-883a-fa2e4b6f9a21\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-196c9471-8526-4ca5-883a-fa2e4b6f9a21\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4pRx0p0YFjc"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "N4pRx0p0YFjc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "485b822f",
        "outputId": "b0f1972b-bf05-4f5b-f081-f04b8a4625c5"
      },
      "source": [
        "# Verify Kaggle CLI is ready\n",
        "!kaggle --help | head -n 20"
      ],
      "id": "485b822f",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CddoBFyhYFjd"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?\n",
        "\n",
        "So that only the owner of the file can read and write to it. API tokens grant access to your account so you are avoiding the risk of unauthroized access by attackers, accidental exposure, and credential compromise from someone else performing action on your behalf."
      ],
      "id": "CddoBFyhYFjd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VyM9dvdYFjd"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "6VyM9dvdYFjd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPDyT1NKYFje"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "YPDyT1NKYFje"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBoK6I2kYFje"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "# # !mkdir -p /content/data/raw\n",
        "# # !kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "# # !unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# # # List CSV inventory\n",
        "# # !ls -lh /content/data/raw/*.csv"
      ],
      "id": "qBoK6I2kYFje"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785a5c7c",
        "outputId": "3447f600-b45c-43d4-8835-fb46f1a57818"
      },
      "source": [
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using the Kaggle CLI to /content/data\n",
        "# The -p flag specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded file into the raw data directory\n",
        "# The -o flag allows overwriting existing files\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a human-readable format\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "785a5c7c",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 562MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ApUOV7_YFje"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "4ApUOV7_YFje"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08273b16",
        "outputId": "e15c0c1c-1548-4713-8181-02563fb36af7"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "\n",
        "# Assert that there are exactly 6 CSV files\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\"\n",
        "\n",
        "print(\"Found exactly 6 CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(os.path.basename(csv_file))"
      ],
      "id": "08273b16",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found exactly 6 CSV files:\n",
            "recommendation_logs.csv\n",
            "search_logs.csv\n",
            "movies.csv\n",
            "watch_history.csv\n",
            "reviews.csv\n",
            "users.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K20uBSIVYFje"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?\n",
        "\n",
        "So that there is a clean recoord of the raw data we start working with, and that we can easily reproduce results by using the same input data. It's also a good practice for easier troubleshooting and documentation."
      ],
      "id": "K20uBSIVYFje"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65rfUA9IYFje"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "65rfUA9IYFje"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6k6lhtqYFje"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "K6k6lhtqYFje"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF3GCAbXYFje"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — GCS staging (commented)\n",
        "# # import uuid, os\n",
        "# # bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "# # os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "# # !gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "# # !gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "# # print(\"Bucket:\", bucket_name)\n",
        "# # # Verify contents\n",
        "# # !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "HF3GCAbXYFje"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ed0743b",
        "outputId": "c3854ae4-1135-4a7e-fd07-10fee3210196"
      },
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# Generate a unique bucket name with a random suffix\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Create the GCS bucket in the specified region\n",
        "print(f\"Creating bucket: {bucket_name} in region: {os.environ['REGION']}\")\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "\n",
        "# Upload all CSV files from the raw data directory to the 'netflix' folder in the bucket\n",
        "print(f\"Uploading CSVs to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name for verification\n",
        "print(\"\\nSuccessfully created bucket and uploaded files.\")\n",
        "print(\"Bucket Name:\", bucket_name)\n",
        "\n",
        "# Explain the benefits of staging data in GCS\n",
        "print(\"\"\"\n",
        "Benefits of staging data in GCS:\n",
        "- **Centralized Storage:** Provides a single, accessible location for your data.\n",
        "- **Version Control:** GCS offers object versioning, allowing you to track changes and revert if needed.\n",
        "- **Scalability and Durability:** GCS is highly scalable and designed for high durability.\n",
        "- **Integration with GCP Services:** Seamlessly integrates with services like BigQuery, Dataproc, and AI Platform.\n",
        "- **Cost-Effective:** Generally cost-effective for storing large amounts of data.\n",
        "- **Reproducibility:** Provides a stable and addressable source for data, improving workflow reproducibility.\n",
        "\"\"\")"
      ],
      "id": "2ed0743b",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: mgmt467-netflix-61e49b23 in region: us-central1\n",
            "Creating gs://mgmt467-netflix-61e49b23/...\n",
            "Uploading CSVs to gs://mgmt467-netflix-61e49b23/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-61e49b23/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-61e49b23/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-61e49b23/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-61e49b23/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-61e49b23/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-61e49b23/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 60.8MiB/s\n",
            "\n",
            "Successfully created bucket and uploaded files.\n",
            "Bucket Name: mgmt467-netflix-61e49b23\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Centralized Storage:** Provides a single, accessible location for your data.\n",
            "- **Version Control:** GCS offers object versioning, allowing you to track changes and revert if needed.\n",
            "- **Scalability and Durability:** GCS is highly scalable and designed for high durability.\n",
            "- **Integration with GCP Services:** Seamlessly integrates with services like BigQuery, Dataproc, and AI Platform.\n",
            "- **Cost-Effective:** Generally cost-effective for storing large amounts of data.\n",
            "- **Reproducibility:** Provides a stable and addressable source for data, improving workflow reproducibility.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq52NGygYFjf"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "eq52NGygYFjf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf089b5f",
        "outputId": "e53e6772-fa45-48f6-8737-0942591649a2"
      },
      "source": [
        "# List the contents of the netflix/ prefix in the GCS bucket with object sizes\n",
        "!gcloud storage ls -l gs://$BUCKET_NAME/netflix/"
      ],
      "id": "cf089b5f",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    115942  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/movies.csv\n",
            "   4695557  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/recommendation_logs.csv\n",
            "   1861942  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/reviews.csv\n",
            "   2250902  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/search_logs.csv\n",
            "   1606820  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/users.csv\n",
            "   9269425  2025-10-09T19:40:20Z  gs://mgmt467-netflix-61e49b23/netflix/watch_history.csv\n",
            "TOTAL: 6 objects, 19800588 bytes (18.88MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJSHqJu6YFjf"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab.\n",
        "\n",
        "1. Loading data into BigQuery directly from GCS is faster and more scalable, especially for large datasets.\n",
        "2. Staging in GCP is also better becaus eyou are loading from a stable location instead of temporary file on a locak machine, so the loading process is more reproducible and easier to audit."
      ],
      "id": "nJSHqJu6YFjf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQhzYqIwYFjf"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "SQhzYqIwYFjf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRaffDXOYFjf"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "VRaffDXOYFjf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTlt828FYFjf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "sTlt828FYFjf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZlXUe7YFjg"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Load tables (commented)\n",
        "# # tables = {\n",
        "# #   \"users\": \"users.csv\",\n",
        "# #   \"movies\": \"movies.csv\",\n",
        "# #   \"watch_history\": \"watch_history.csv\",\n",
        "# #   \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "# #   \"search_logs\": \"search_logs.csv\",\n",
        "# #   \"reviews\": \"reviews.csv\",\n",
        "# # }\n",
        "# # import os\n",
        "# # for tbl, fname in tables.items():\n",
        "# #   src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "# #   print(\"Loading\", tbl, \"from\", src)\n",
        "# #   !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $DATASET.$tbl $src\n",
        "# #\n",
        "# # # Row counts\n",
        "# # for tbl in tables.keys():\n",
        "# #   !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "s2ZlXUe7YFjg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "179763b2",
        "outputId": "856a3a86-3c3b-4213-f4a9-aaeb12032ea1"
      },
      "source": [
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists and print a friendly message\n",
        "!bq --location=US mk -d --description \"Netflix dataset for MGMT467\" $DATASET 2> /dev/null || echo \"Dataset '$DATASET' may already exist.\""
      ],
      "id": "179763b2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'boxwood-veld-471119-r6:netflix' successfully created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e2d9a63",
        "outputId": "6636e60c-e233-4236-e3c9-47360585350f"
      },
      "source": [
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "import os\n",
        "DATASET=\"netflix\" # Ensure DATASET is defined\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"] # Get PROJECT_ID from environment\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(f\"Loading {tbl} from {src}\")\n",
        "  # Use bq load command with specified flags\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts for verification\n",
        "print(\"\\nRow counts for loaded tables:\")\n",
        "for tbl in tables.keys():\n",
        "  # Escape the backticks with backslashes\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM \\`{PROJECT_ID}.{DATASET}.{tbl}\\`\""
      ],
      "id": "7e2d9a63",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-61e49b23/netflix/users.csv\n",
            "Waiting on bqjob_ref15040e8af8b8b_00000199ca92971e_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-61e49b23/netflix/movies.csv\n",
            "Waiting on bqjob_r2237168453284a19_00000199ca92ada2_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-61e49b23/netflix/watch_history.csv\n",
            "Waiting on bqjob_r70a7ab0c1aabe68e_00000199ca92c56b_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-61e49b23/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r2d10d32f905ded70_00000199ca92e0d3_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-61e49b23/netflix/search_logs.csv\n",
            "Waiting on bqjob_r2538b176059f26cf_00000199ca92fa80_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-61e49b23/netflix/reviews.csv\n",
            "Waiting on bqjob_rf8b4c504e84aba8_00000199ca93109c_1 ... (1s) Current status: DONE   \n",
            "\n",
            "Row counts for loaded tables:\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| users      | 51500 |\n",
            "+------------+-------+\n",
            "+------------+------+\n",
            "| table_name |  n   |\n",
            "+------------+------+\n",
            "| movies     | 5200 |\n",
            "+------------+------+\n",
            "+---------------+--------+\n",
            "|  table_name   |   n    |\n",
            "+---------------+--------+\n",
            "| watch_history | 525000 |\n",
            "+---------------+--------+\n",
            "+---------------------+--------+\n",
            "|     table_name      |   n    |\n",
            "+---------------------+--------+\n",
            "| recommendation_logs | 260000 |\n",
            "+---------------------+--------+\n",
            "+-------------+--------+\n",
            "| table_name  |   n    |\n",
            "+-------------+--------+\n",
            "| search_logs | 132500 |\n",
            "+-------------+--------+\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| reviews    | 77250 |\n",
            "+------------+-------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYXFdJWYFjg"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "vBYXFdJWYFjg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fc6b22",
        "outputId": "e67319fc-2193-42ac-fa5e-7035267a0e7d"
      },
      "source": [
        "# Query BigQuery metadata to get row counts for all tables in the dataset\n",
        "!bq query --nouse_legacy_sql 'SELECT table_id AS table_name, row_count FROM `{os.environ[\"GOOGLE_CLOUD_PROJECT\"]}.netflix.__TABLES__` WHERE table_id IN (\"users\", \"movies\", \"watch_history\", \"recommendation_logs\", \"search_logs\", \"reviews\")'"
      ],
      "id": "41fc6b22",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r26b9c124e03daa2f_00000199ca939851_1 ... (0s) Current status: DONE   \n",
            "+---------------------+-----------+\n",
            "|     table_name      | row_count |\n",
            "+---------------------+-----------+\n",
            "| movies              |      5200 |\n",
            "| recommendation_logs |    260000 |\n",
            "| reviews             |     77250 |\n",
            "| search_logs         |    132500 |\n",
            "| users               |     51500 |\n",
            "| watch_history       |    525000 |\n",
            "+---------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RELVJ1c5YFjh"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?\n",
        "\n",
        "Autodetect is acceptable in initial exploration when you are not fully sure of the schema yet. It is also okey when you data source has a very predictable structure where autodetect can correctly infer the schema.\n",
        "\n",
        "Though you should enforce explicit schemas when producing data pipelines and critical datasets to ensure data quality and consistency without incorrect type inference errors. Autodetect can also sometimes make incorrect inferences, especially with mixed data types or ambiguous date formats. Hence explicit schemas give you control over how these situations are handled."
      ],
      "id": "RELVJ1c5YFjh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbd60LUeYFjh"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "Lbd60LUeYFjh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLID5ujPYFjh"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "gLID5ujPYFjh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13mM0AXrYFjh"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "13mM0AXrYFjh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1d050H-YFji"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "h1d050H-YFji"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHT3bjmmYFji"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "VHT3bjmmYFji"
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's check what columns actually exist in the users table\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "# Check the schema of the users table\n",
        "!bq show --schema --format=prettyjson {project_id}:netflix.users"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etf-w1BylPuo",
        "outputId": "46cd88a5-779f-431e-c9b7-be4aad42df85"
      },
      "id": "Etf-w1BylPuo",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"email\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"first_name\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"last_name\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"age\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"gender\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"country\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"state_province\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"city\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"subscription_plan\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"subscription_start_date\",\n",
            "    \"type\": \"DATE\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"is_active\",\n",
            "    \"type\": \"BOOLEAN\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"monthly_spend\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"primary_device\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"household_size\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"created_at\",\n",
            "    \"type\": \"TIMESTAMP\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "536f0493",
        "outputId": "f8ac6eac-03be-4b46-87db-568ae81b9cd7"
      },
      "source": [
        "# Cell 1: Total rows and % missing in country, subscription_plan, age\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM \\\\`{project_id}.netflix.users\\\\`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "536f0493",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "|   n   | pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "| 51500 |                 0.0 |                           0.0 |           11.93 |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1362f40c",
        "outputId": "f88623bc-6a19-4429-9191-7d86cbc35e0d"
      },
      "source": [
        "# Cell 2: % subscription_plan missing by country (ordered descending)\n",
        "# Investigate if missingness depends on another variable (country) - potential MAR\n",
        "# MAR (Missing At Random): If subscription_plan missingness varies by country,\n",
        "# it suggests the probability of being missing depends on the observed country value.\n",
        "# This would indicate MAR rather than MCAR (Missing Completely At Random).\n",
        "import os\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT country,\n",
        "       COUNT(*) AS n,\n",
        "       ROUND(100*COUNTIF(subscription_plan IS NULL)/COUNT(*),2) AS pct_missing_subscription_plan\n",
        "FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users\\\\`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_subscription_plan DESC\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "1362f40c",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+-------------------------------+\n",
            "| country |   n   | pct_missing_subscription_plan |\n",
            "+---------+-------+-------------------------------+\n",
            "| USA     | 36020 |                           0.0 |\n",
            "| Canada  | 15480 |                           0.0 |\n",
            "+---------+-------+-------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq4355rxYFji"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "oq4355rxYFji"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ebb54cc",
        "outputId": "ae2ba4ce-bd00-4ff5-ed80-bb7df41890d6"
      },
      "source": [
        "# Verification: Print the three missingness percentages\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM \\\\`{project_id}.netflix.users\\\\`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "9ebb54cc",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------------------+-----------------+\n",
            "| pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+---------------------+-------------------------------+-----------------+\n",
            "|                 0.0 |                           0.0 |           11.93 |\n",
            "+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8q1tcycYFjj"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why.\n",
        "\n",
        "Age columns has the most missing values with 11.93% missing, country and subscription_plan have 0% missing.\n",
        "\n",
        "**MCAR:** Could be MCAR if the missingness is completely random, could be random system error or data entry issue.\n",
        "\n",
        "**MAR:** It could be MAR if the missingness is related to other observed variables. For example, maybe users with a certain subscription_plan or from a specific country are less likely to provide their age.\n",
        "\n",
        "**MNAR:** It could be MNAR if the missingness is related to the unobserved age value itself. For example, maybe users who are very young or very old are less likely to report their age. This is a common scenario with sensitive demographic data."
      ],
      "id": "F8q1tcycYFjj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pkq2l2PYFjj"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "_Pkq2l2PYFjj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1w76WMsYFjk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "g1w76WMsYFjk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZZR5hhHYFjk"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "8ZZR5hhHYFjk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7jNshBeYFjs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "X7jNshBeYFjs"
    },
    {
      "cell_type": "code",
      "source": [
        "# First, check the actual column names in watch_history\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "!bq show --schema --format=prettyjson {project_id}:netflix.watch_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTe7XFmNs-ye",
        "outputId": "eff11145-e935-4b00-8241-37d22f352105"
      },
      "id": "RTe7XFmNs-ye",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"session_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"movie_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"watch_date\",\n",
            "    \"type\": \"DATE\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"device_type\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"watch_duration_minutes\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"progress_percentage\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"action\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"quality\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"location_country\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"is_download\",\n",
            "    \"type\": \"BOOLEAN\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_rating\",\n",
            "    \"type\": \"INTEGER\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Report duplicate groups on (user_id, movie_id, watch_date, device_type) with counts (top 20)\n",
        "# This identifies rows that have identical combinations of these four columns\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT user_id,\n",
        "       movie_id,\n",
        "       watch_date,\n",
        "       device_type,\n",
        "       COUNT(*) AS duplicate_count\n",
        "FROM \\\\`{project_id}.netflix.watch_history\\\\`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING COUNT(*) > 1\n",
        "ORDER BY duplicate_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwhE7Ag5ss7Q",
        "outputId": "ac90d092-b711-43a2-bf18-1250e9f1f929"
      },
      "id": "QwhE7Ag5ss7Q",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+------------+-------------+-----------------+\n",
            "|  user_id   |  movie_id  | watch_date | device_type | duplicate_count |\n",
            "+------------+------------+------------+-------------+-----------------+\n",
            "| user_03310 | movie_0640 | 2024-09-08 | Smart TV    |              20 |\n",
            "| user_00391 | movie_0893 | 2024-08-26 | Laptop      |              20 |\n",
            "| user_03898 | movie_0500 | 2025-07-29 | Desktop     |              15 |\n",
            "| user_04513 | movie_0564 | 2024-06-11 | Mobile      |              15 |\n",
            "| user_03043 | movie_0465 | 2024-02-03 | Laptop      |              15 |\n",
            "| user_08157 | movie_0729 | 2025-10-26 | Laptop      |              15 |\n",
            "| user_06554 | movie_0505 | 2025-10-02 | Laptop      |              15 |\n",
            "| user_01580 | movie_0984 | 2025-06-25 | Mobile      |              15 |\n",
            "| user_02976 | movie_0987 | 2024-09-19 | Desktop     |              15 |\n",
            "| user_03408 | movie_0146 | 2025-06-02 | Desktop     |              15 |\n",
            "| user_05629 | movie_0697 | 2025-01-23 | Desktop     |              15 |\n",
            "| user_04050 | movie_0898 | 2025-07-05 | Mobile      |              15 |\n",
            "| user_01303 | movie_0858 | 2025-09-03 | Mobile      |              15 |\n",
            "| user_01870 | movie_0844 | 2024-06-02 | Laptop      |              15 |\n",
            "| user_01182 | movie_0794 | 2025-07-03 | Desktop     |              15 |\n",
            "| user_03460 | movie_0526 | 2024-03-16 | Smart TV    |              15 |\n",
            "| user_02028 | movie_0037 | 2024-08-13 | Desktop     |              15 |\n",
            "| user_06417 | movie_0590 | 2024-01-15 | Laptop      |              15 |\n",
            "| user_09454 | movie_0116 | 2025-10-19 | Laptop      |              15 |\n",
            "| user_09972 | movie_0536 | 2025-07-16 | Laptop      |              15 |\n",
            "+------------+------------+------------+-------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Create deduplicated watch_history table\n",
        "# Strategy: Keep one row per (user_id, movie_id, watch_date, device_type) group\n",
        "# Preference: Higher progress_percentage first, then higher watch_duration_minutes\n",
        "# This removes duplicates while preserving the most complete viewing record\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \\\\`{project_id}.netflix.watch_history_dedup\\\\` AS\n",
        "SELECT * EXCEPT(row_num)\n",
        "FROM (\n",
        "  SELECT *,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS row_num\n",
        "  FROM \\\\`{project_id}.netflix.watch_history\\\\`\n",
        ")\n",
        "WHERE row_num = 1\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yo2L1BztMLs",
        "outputId": "bee6b4ba-b046-4fe3-f9a2-507f6a3262bc"
      },
      "id": "0Yo2L1BztMLs",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r49f7a0312d565bfc_00000199cab4f41d_1 ... (1s) Current status: DONE   \n",
            "Created boxwood-veld-471119-r6.netflix.watch_history_dedup\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITwGxezmYFjt"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "ITwGxezmYFjt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: Compare row counts before and after deduplication\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  'Original' AS table_name,\n",
        "  COUNT(*) AS row_count\n",
        "FROM \\\\`{project_id}.netflix.watch_history\\\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'Deduplicated' AS table_name,\n",
        "  COUNT(*) AS row_count\n",
        "FROM \\\\`{project_id}.netflix.watch_history_dedup\\\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'Duplicates Removed' AS table_name,\n",
        "  (SELECT COUNT(*) FROM \\\\`{project_id}.netflix.watch_history\\\\`) -\n",
        "  (SELECT COUNT(*) FROM \\\\`{project_id}.netflix.watch_history_dedup\\\\`) AS row_count\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFGuklaQtcQG",
        "outputId": "e7a9ee67-3b68-453b-feb8-65c1eeaaf52b"
      },
      "id": "sFGuklaQtcQG",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|     table_name     | row_count |\n",
            "+--------------------+-----------+\n",
            "| Original           |    525000 |\n",
            "| Duplicates Removed |    425000 |\n",
            "| Deduplicated       |    100000 |\n",
            "+--------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h1z1f7DYFjt"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?\n",
        "\n",
        "**Natural duplicates** occur when users legitimately engage with content multiple times. For example, watching the same movie in different sessions throughout the day, switching between devices mid-viewing, or multiple household members using a shared account. **System-generated duplicate** on the other hand stem from technical problems like ETL pipeline failures that reload data multiple times, application bugs that log events redundantly, network retry logic creating duplicate API calls, or improper data synchronization across multiple sources without idempotency checks.\n",
        "\n",
        "Duplicates distort labels and KPIS because it inflates engagement statistics.If one person watches a movie but it gets recorded three times, that's 300% fake engagement. This breaks recommendation algorithms because they think certain movies are more popular than they actually are, leading to bad suggestions."
      ],
      "id": "4h1z1f7DYFjt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMEofZAYFju"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "zmMEofZAYFju"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWGn1q0JYFju"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "qWGn1q0JYFju"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eztiWxfYFju"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "# # WITH dist AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # bounds AS (\n",
        "# #   SELECT q1, q3, (q3-q1) AS iqr,\n",
        "# #          q1 - 1.5*(q3-q1) AS lo,\n",
        "# #          q3 + 1.5*(q3-q1) AS hi\n",
        "# #   FROM dist\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "# # CROSS JOIN bounds b;"
      ],
      "id": "0eztiWxfYFju"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH0S7q_wYFju"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "# # WITH q AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   h.*,\n",
        "# #   GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "# #\n",
        "# # -- Quantiles before vs after\n",
        "# # WITH before AS (\n",
        "# #   SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # after AS (\n",
        "# #   SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "# # )\n",
        "# # SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "jH0S7q_wYFju"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b69a22a0",
        "outputId": "495b57c3-6c93-4ace-f213-6b24b662233e"
      },
      "source": [
        "# Compute IQR bounds and report % outliers for watch_duration_minutes\n",
        "import os\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\` h\n",
        "CROSS JOIN bounds b\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "b69a22a0",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------------+\n",
            "| outliers | total  | pct_outliers |\n",
            "+----------+--------+--------------+\n",
            "|     3433 | 100000 |         3.43 |\n",
            "+----------+--------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7330aef7",
        "outputId": "a6bbc318-c358-40b0-9fc6-0438bb7152c1"
      },
      "source": [
        "# Create watch_history_robust with watch_duration_minutes_capped at P01/P99\n",
        "# Return quantile summaries before/after capping\n",
        "import os\n",
        "\n",
        "sql_create_table = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\\\` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\` h, q\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_create_table}\"\n",
        "\n",
        "print(\"\\nQuantiles before vs after capping:\")\n",
        "\n",
        "sql_quantiles = f\"\"\"\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\\\`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_quantiles}\""
      ],
      "id": "7330aef7",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r5221cdbad57c2fff_00000199cac04207_1 ... (2s) Current status: DONE   \n",
            "Created boxwood-veld-471119-r6.netflix.watch_history_robust\n",
            "\n",
            "\n",
            "Quantiles before vs after capping:\n",
            "+--------+---------------------------------------------+\n",
            "| which  |                      q                      |\n",
            "+--------+---------------------------------------------+\n",
            "| before | [\"0.2\",\"24.9\",\"41.7\",\"61.4\",\"91.7\",\"799.3\"] |\n",
            "| after  | [\"4.4\",\"24.6\",\"41.5\",\"61.5\",\"92.0\",\"366.0\"] |\n",
            "+--------+---------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mIvwwVaYFjv"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "8mIvwwVaYFjv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: Show min/median/max before vs after capping\n",
        "import os\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  'Before (Original)' AS version,\n",
        "  MIN(watch_duration_minutes) AS min_val,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_val,\n",
        "  MAX(watch_duration_minutes) AS max_val\n",
        "FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'After (Capped)' AS version,\n",
        "  MIN(watch_duration_minutes_capped) AS min_val,\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_val,\n",
        "  MAX(watch_duration_minutes_capped) AS max_val\n",
        "FROM \\\\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\\\`\n",
        "\n",
        "ORDER BY version DESC\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aUFlOT-wHWg",
        "outputId": "10a0dec5-e09c-4613-a73e-5e70aeec27a9"
      },
      "id": "3aUFlOT-wHWg",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+------------+---------+\n",
            "|      version      | min_val | median_val | max_val |\n",
            "+-------------------+---------+------------+---------+\n",
            "| Before (Original) |     0.2 |       51.2 |   799.3 |\n",
            "| After (Capped)    |     4.4 |       51.4 |   366.0 |\n",
            "+-------------------+---------+------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJmspmvvYFjv"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why.\n",
        "\n",
        "Capping might be harmfull if the outliers are actually meaningful and represent significant events. If the data naturally follows a distribution with extreme values, then capping might also misrepresent the true nature of the data and lead to biased conclusions\n",
        "\n",
        "Decision trees and random forests are less sensetive to outliers as they work by recursively partitioning the data based on feature values. When splitting a node, they look for a threshold that best separates the data, so outliers might end up in their own small leaf nodes or split, but won't influence overall structure of the tree.\n"
      ],
      "id": "oJmspmvvYFjv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjx9-lHRYFjv"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "Xjx9-lHRYFjv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjD-IdFGYFjv"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "JjD-IdFGYFjv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4bMWu45YFjw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(minutes_watched > 8*60)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "d4bMWu45YFjw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPLBgRlBYFjw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #           CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #                     CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "dPLBgRlBYFjw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLCrxw3RYFjw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "# #   COUNTIF(duration_min > 8*60) AS titles_over_8h,\n",
        "# #   COUNT(*) AS total\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "MLCrxw3RYFjw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Create and summarize flag_binge for sessions > 8 hours (480 minutes)\n",
        "# Identifies potential binge-watching behavior in watch_history_robust\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 480) AS binge_sessions,\n",
        "  COUNT(*) AS total_sessions,\n",
        "  ROUND(100 * COUNTIF(watch_duration_minutes_capped > 480) / COUNT(*), 2) AS pct_binge,\n",
        "  AVG(CASE WHEN watch_duration_minutes_capped > 480 THEN watch_duration_minutes_capped END) AS avg_binge_duration,\n",
        "  MAX(watch_duration_minutes_capped) AS max_binge_duration\n",
        "FROM \\\\`{project_id}.netflix.watch_history_robust\\\\`\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-y26wyexeao",
        "outputId": "9ae2dc2d-34d4-4675-95d3-ab1146e99de0"
      },
      "id": "n-y26wyexeao",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------+-----------+--------------------+--------------------+\n",
            "| binge_sessions | total_sessions | pct_binge | avg_binge_duration | max_binge_duration |\n",
            "+----------------+----------------+-----------+--------------------+--------------------+\n",
            "|              0 |         100000 |       0.0 |               NULL |              366.0 |\n",
            "+----------------+----------------+-----------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Create and summarize flag_age_extreme for users with age <10 or >100\n",
        "# Flag extreme/suspicious ages that may indicate data quality issues\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_users,\n",
        "  COUNT(*) AS total_users_with_age,\n",
        "  ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct_extreme_age,\n",
        "  COUNTIF(age < 10) AS users_under_10,\n",
        "  COUNTIF(age > 100) AS users_over_100,\n",
        "  MIN(age) AS min_age,\n",
        "  MAX(age) AS max_age\n",
        "FROM \\\\`{project_id}.netflix.users\\\\`\n",
        "WHERE age IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDYeMpd8xiXG",
        "outputId": "f52255eb-6478-4a69-fff7-bd2a28f363e6"
      },
      "id": "wDYeMpd8xiXG",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n",
            "| extreme_age_users | total_users_with_age | pct_extreme_age | users_under_10 | users_over_100 | min_age | max_age |\n",
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n",
            "|               895 |                45355 |            1.97 |            840 |             55 |    -7.0 |   109.0 |\n",
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeb15add",
        "outputId": "16cdadda-26ce-4d4b-db1d-a9a98171bde9"
      },
      "source": [
        "# Cell 3: Compute and summarize flag_duration_anomaly for movies with durations < 15 or > 480 minutes\n",
        "# Flag movies with potentially anomalous durations (very short or very long)\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS duration_anomalies,\n",
        "  COUNT(*) AS total_movies,\n",
        "  ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2)\n",
        "    AS pct_duration_anomalies,\n",
        "  COUNTIF(duration_minutes < 15)  AS movies_under_15_min,\n",
        "  COUNTIF(duration_minutes > 480) AS movies_over_480_min\n",
        "FROM \\\\`{project_id}.netflix.movies\\\\`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "aeb15add",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n",
            "| duration_anomalies | total_movies | pct_duration_anomalies | movies_under_15_min | movies_over_480_min |\n",
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n",
            "|                115 |         5200 |                   2.21 |                  60 |                  55 |\n",
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm2wpQwcYFjx"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "Sm2wpQwcYFjx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e07c28ab",
        "outputId": "5292be40-79db-4650-ec41-dd795f70618f"
      },
      "source": [
        "# Compact summary of all data quality and behavioral flags (binge, age, duration)\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT 'flag_binge' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(watch_duration_minutes_capped > 480) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\\\`{project_id}.netflix.watch_history_robust\\\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT 'flag_age_extreme' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\\\`{project_id}.netflix.users\\\\`\n",
        "WHERE age IS NOT NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT 'flag_duration_anomaly' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\\\`{project_id}.netflix.movies\\\\`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\"\n",
        "\n"
      ],
      "id": "e07c28ab",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-------------+\n",
            "|       flag_name       | pct_of_rows |\n",
            "+-----------------------+-------------+\n",
            "| flag_binge            |         0.0 |\n",
            "| flag_age_extreme      |        1.97 |\n",
            "| flag_duration_anomaly |        2.21 |\n",
            "+-----------------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i004zUrYFjx"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?\n",
        "\n",
        "`flag_duration_anomaly` is the most common anomaly flag with 2.21% of movies having durations outside the typical range. I would keep this flag as a data quality feature since it reflects unusual content characteristics that can affect recommendation accuracy. However, I would also keep `flag_binge` as a feature because it captures a highly relevant user behavior which that can help predict engagement, churn, or subscription upgrades. Even though it currently shows 0.0%, it remains valuable for future datasets where binge sessions are more common."
      ],
      "id": "_i004zUrYFjx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ww2ciqmYFjy"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "4ww2ciqmYFjy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlNxyt5fYFjy"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "JlNxyt5fYFjy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe88m6oXYFjy"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "Xe88m6oXYFjy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
